---
layout: post
title: entropy trong lý thuyết thông tin
subtitle: Từ thống kê đến học máy
# color: turquoise
feature-img: assets/dl_posts/entropy/entropy4.jpg
excerpt_separator: 
categories: [Neural Networks]
tags: [statistics, machine learning, entropy, cross-entropy]
date: June 26, 2020 
comments: true
mathjax: true
---
Cảm hứng đầu tiên đưa tôi đến bài viết này là một bài viết xuất sắc của của anh Lê Quang Tiến, nhan đề: [Entropy, Cross Entropy, KL Divergence](https://thetalog.com/statistics/ly-thuyet-thong-tin/). Tôi tin là chỉ với bài này, bạn không cần phải đọc thêm bài viết dưới của tôi. Nhưng nếu bạn muốn có một khảo cứu sâu hơn về vấn đề `entropy`, thì chúng ta cùng bắt đầu thôi.

**Nội dung bài viết**

* TOC
{:toc}

"Anything that can go wrong, will go wrong" - Murphy. [Murphy's law](https://en.wikipedia.org/wiki/Murphy%27s_law).

Tạm dịch: "Bất cứ thứ gì có thể trở nên tồi tệ, thì sẽ trở nên tồi tệ".

{% include aligner.html images="dl_posts/entropy/entropy2.png,dl_posts/entropy/entropy1.jpg" %}

Định luật trên là sự ám chỉ về xu hướng gây mất trật tự và làm mọi chuyện trở nên khó khăn trong đời sống của xã hội. Mọi vấn đề sẽ luôn phát sinh một cách tự nhiên lên chính bản thân của một người, muốn giải quyết vấn đề, nhất định phải cần năng lượng, trí tuệ, công sức, tiền bạc đổ vào đó. Cuộc sống không tự duy trì một cách hệ thống, mà có xu hướng ngày một trở nên phức tạp. Nếu không làm chủ được chính bản thân mình và điều hòa được cuộc sống, mọi thứ sẽ tệ dần đi theo thời gian.

Định luật trên thực sự là một vấn đề triết học, bao trùm lên nhiều ngành khoa học, nó đồng thời liên quan đến một trong những lực lượng lớn nhất trong vũ trụ, là căn bản của thế giới, là lực lượng chi phối cuộc đời mỗi con người, dẫn dắt nhiều vấn đề ta hay đối mặt và đương nhiên, nó đưa đến sự hỗn loạn: `entropy`.

Time to music: [Hỗn Độn](https://www.youtube.com/watch?v=52bqXTFRJfw).

# Sự ra đời của entropy

## Tên gọi entropy
Khi bắt đầu học về Machine Learning, tôi mới biết đến entropy thông qua một hàm số, gọi là `cross-entropy` mà lát nữa tôi sẽ đề cập. Thực sự thì khi tìm hiểu về entropy, tôi cũng rất mơ hồ vì khái niệm này quá lạ lẫm với tôi, có lẽ với những người học về vật lý lý thuyết thì sẽ quen hơn là những người làm toán. Lý do là vì entropy xuất hiện đầu tiên trong nhiệt động lực học, cụ thể: *"Lịch sử của entropy bắt đầu với công trình của nhà toán học người Pháp Lazare Carnot, quyển Các nguyên lý cơ bản của cân bằng và chuyển động (1803). Trong tác phẩm này, ông đã đề xuất nguyên lý cho rằng tất cả những sự gia tốc và va chạm của các phần đang chuyển động trong mọi cơ cấu đều có hiện diện của những hao tổn về "moment hoạt động". "* Chi tiết hơn, bạn có thể đọc ở [đây](https://vi.wikipedia.org/wiki/Entropy).

Theo tiếng Hy Lạp, “tropy” (τροπή ), có nghĩa là thay đổi, chuyển hướng, còn “en” có nghĩa là “bên trong”. Khi phiên âm sang tiếng Việt thì entropy trở thành ên-trô-pỳ. Như vậy, entropy là "kẻ gây ra sự xáo trộn, kẻ gây hỗn độn, hỗn loạn". Nếu coi vũ trụ này là một trường học thì entropy là học sinh cá biệt trong trường học đó.

 Nếu muốn tìm hiểu kĩ càng hơn, tôi rất khuyến khích bạn đọc bài báo [A Review of the Entropy Concept](https://arxiv.org/pdf/1711.07326.pdf#:~:text=Roots%20and%20genesis%20of%20the%20entropy%20concept&text=Two%20kinds%20of%20entropy%3B%20thermodynamic,above%20absolute%20zero%20%5B117%5D.), nó sẽ cho bạn một cái nhìn khoa học, trực diện và tổng quan về entropy. 

## Muôn hình vạn trạng của entropy

Vì sao việc dọn rửa nhà cửa lại phải diễn ra thường xuyên? Vì sao luôn có những kẻ ngáng chân trên con đường của bạn? Vì sao gia đình bạn thỉnh thoảng lại lục đục nội bộ? Vì sao trộm cắp ngày một nhiều dù cho xã hội "ngày một văn minh"?,... Câu trả lời chung nhất cho tất cả điều trên: Là vì entropy.

- Rất thú vị, trong một bài viết về văn hóa, tác giả PV. Hưng - một người thầy tôi ngưỡng mộ, đã viết về `entropy xã hội` là một kiểu mô tả những hỗn loạn ngày càng tăng trong xã hội loài người [(link bài viết)](https://viethungpham.com/2015/06/09/from-entropy-to-the-tao-tu-entropy-den-dao/).
Cũng trong bài viết trên, tác giả có nói đến hai định nghĩa thêm về entropy
    - Từ điển Dictionary of the English Language do Houghton Mifflin Harcourt xuất bản tại Mỹ năm 2011, đưa ra 5 định nghĩa của entropy, trong đó có định nghĩa sau đây: “Entropy: Inevitable and steady deterioration of a system or society” (Entropy: Sự suy thoái chắc chắn và không thể tránh khỏi của một hệ thống hoặc một xã hội).
    - Từ điển Random House Kernerman Webster’s College đưa ra 4 định nghĩa, trong đó định nghĩa thứ 4 viết: “Entropy: a state of disorder, as in a social system, or a hypothetical tendency toward such a state” (Entropy: một trạng thái hỗn loạn, như trong một hệ thống xã hội, hoặc một xu thế được cho là hướng tới một trạng thái như thế).

Thực sự thì bài viết trên quá xuất sắc nên tôi cũng không đi sâu thêm nữa, bạn hãy đọc và tự cảm nhận. Tôi sẽ chỉ đưa thêm ba loại entropy dưới đây
- `entropy trong nhiệt động lực học`, ký hiệu là \\(\displaystyle S\\), là một đơn vị đo nhiệt năng phát tán, hấp thụ khi một hệ vật lý chuyển trạng thái tại một nhiệt độ tuyệt đối xác định.
\\[dS = \dfrac{\partial Q}{T},\\] trong đó \\(S\\) là độ lớn của entropy, \\(dS\\) là sự thay đổi độ lớn của entropy, \\(T\\) là nhiệt độ tuyệt đối, còn \\(\partial Q\\) là độ thay đổi nhiệt năng của hệ.

Theo [định luật 2 nhiệt động lực học](https://vi.wikipedia.org/wiki/%C4%90%E1%BB%8Bnh_lu%E1%BA%ADt_hai_nhi%E1%BB%87t_%C4%91%E1%BB%99ng_l%E1%BB%B1c_h%E1%BB%8Dc), chỗ nhận được nhiệt đó phải lạnh hơn là vật ban đầu nếu ta không muốn tiêu thêm năng lượng vào đó. Như vậy, entropy của vũ trụ này luôn tăng theo thời gian. Nếu entropy ở chỗ nào đó giảm đi, thì ở các chỗ xung quanh sẽ phải tăng lên một lượng nhiều hơn lượng giảm đi đó để bù lại, và tính tổng cộng thì vẫn là tăng lên. Đây cũng là một trong những lí lẽ để bảo chứng rằng **không bao giờ có chuyện tạo ra được cỗ máy thời gian quay về quá khứ**, hay thời gian đảo ngược, vì đơn giản là entropy không ngừng tăng.
Ngoài ra, việc entropy luôn luôn "ăn" năng lượng hữu ích, sẽ dẫn đến trạng thái `maximal entropy`, lúc đó sẽ không còn năng lượng có thể sử dụng được, dẫn đến giả thuyết *cái chết nhiệt*, hay [heat death](https://en.wikipedia.org/wiki/Heat_death_of_the_universe). Cần phải xác thực rằng entropy không phải là một dạng năng lượng, nó chỉ "ăn" năng lượng hữu ích, tạo ra năng lượng vô ích và liên tục lớn lên.

Một ví dụ tiêu biểu, tủ lạnh hút nhiệt bên trong tủ thổi ra ngoài, làm bên trong thì lạnh đi, còn bên ngoài nóng lên. Entropy chỉ cho phép tủ lạnh hay máy điều hòa làm lạnh với điều kiện là phải cung cấp cho nó năng lượng: Khi máy điều hòa làm giảm nhiệt năng trong phòng, thì làm tăng nhiệt năng ngoài trời một lượng nhiều hơn là lượng giảm đi trong phòng, và độ chênh lệch giữa hai lượng chính là lượng điện năng mà máy tiêu thụ. Nhiệt độ ngoài trời mà càng cao hơn bên trong nhà, thì càng phải "feed" nhiều điện năng cho entropy, khi đó mới được phép thổi nhiệt ngược từ chỗ lạnh trong nhà ra chỗ nóng ngoài trời.
- `entropy trong cơ học thống kê` được định nghĩa như là một đơn vị đo lường khả năng mà một hệ có thể rơi vào trạng thái hỗn độn trong một tình trạng, nó thường được gọi là "sự lộn xộn" hay "tính bừa" thể hiện trong một hệ.
- Trong kinh tế, khái niệm `corporate entropy` để chỉ sự rắm rối của các doanh nghiệp dẫn đến giảm hiệu quả lao động.
- Loại cuối cùng, cũng chính là chủ đạo trong bài viết này: [entropy trong lí thuyết thông tin](https://vi.wikipedia.org/wiki/Entropy_th%C3%B4ng_tin).

# entropy trong lý thuyết thông tin

*Khăn ăn trang trí bàn, các dải tư duy và các nhóm phương trình rải rác chồng chất quanh ông. Ông viết chi chít trên giấy có dòng kẻ, nhưng những dòng nháp thì ở khắp mọi nơi. Tám năm như vậy – nguệch ngoạc, trau chuốt lại, gạch đi, nhìn chằm chằm vào một nhóm đầy ắp các phương trình, dù biết sau tất cả những nỗ lực ấy, chúng có thể chẳng hé lộ gì hết. Có những khoảng thời gian nghỉ cho thuốc lá và âm nhạc, và người đàn ông mắt mũi còn lơ mơ đi bộ tới chỗ làm, nhưng hầu như đó là quá trình nghiên cứu không ngừng nghỉ. Có lẽ, ông đã cảm nhận rằng mình đang nghiên cứu là thứ gì đó ấn tượng, còn căn bản hơn cả luận văn thạc sĩ đã làm nên tên tuổi của ông,...* (trích từ [câu chuyện đằng sau sự ra đời của kỷ nguyên thông tin – I](https://dzuykhanh.wordpress.com/2018/04/07/claude-shannon/).

Ông là Claude Shannon, cha đẻ của **lý thuyết thông tin**.

{% include aligner.html images="dl_posts/entropy/shannon.jpeg" %}

Trước khi có những công cụ để lưu trữ, diễn giải và truyền đi thông tin thì nó là thứ mà người ta phỏng đoán nhiều hơn là nói về. Từ thời xa xưa, loài người tinh khôn (Homo Sapiens) đã biết dùng lửa để báo hiệu cho nhau, đó là một dạng thông tin ám hiệu. Khi nền văn minh ở lưu vực sông Hoàng Hà phát triển, người Trung Hoa đã biết dùng tre, gỗ để tạo thành thẻ tre phục vụ cho việc ghi chép, đó là dạng thông tin kí tự, chữ viết. Gần đây hơn, thông tin nằm trong mạng lưới bắt nguồn một phần từ những nỗ lực đầu tiên bắc những đường dây cáp ngang biển Đại Tây Dương. Trong nỗ lực tấn công những vấn đề kỹ thuật mang tính thực tiễn như nối hai điểm \\(A\\) và \\(B\\) – số lượng dây nhỏ nhất chúng ta cần treo để xử lý lượng thông điệp hàng ngày là bao nhiêu? Chúng ta làm thế nào để mã hóa những cuộc điện đàm bí mật nhất?

Cùng xét một ví dụ sau: Giả sử trong thời chiến, tiểu đoàn \\(A\\) bị quân địch tấn công. Tiểu đoàn trưởng lúc này cử người đi đưa thư cứu viện, khi đó nội dung bức mật thư sẽ như thế nào để vừa ngắn gọn, vừa đẩy đủ nội dung, lại vừa được khó phát hiện nếu rơi vào tay quân địch?

Có lẽ thay vì: "Chúng tôi bị tấn công, xin gửi quân cứu viện", nên rút bớt thành "cứu viện". Như vậy, nếu trước đó quân đội bên \\(A\\) có một bảng mật mã quy đổi, chẳng hạn 

| Cứu viện      |      \\(0\\)  |
|:-------------:|:-------------:|
| Đánh tiếp     |     \\(1\\)   |
| Lui quân      |   \\(2\\)     |
| \\(\ldots\\)  | \\(\ldots\\)  |

Khi đó, chỉ cần một bức thư với một số \\(0\\) là đủ.

> Lý thuyết thông tin (Information Theory) là một nhánh toán ứng dụng quan tâm đến các vấn đề định lượng (quantification), lưu trữ (storage) và truyền dẫn dữ liệu (communication) của thông tin.

Bởi vì thông tin là một khái niệm trừu tượng (không phải một thực thể lý tính) do đó khó mà định lượng thông tin theo cách thông thường, chúng ta cần những độ đo phù hợp để định lượng chúng.

Đơn vị nhỏ nhất để biểu diễn thông tin gọi là `bit`, đây là chữ viết tắt của `binary digit` hay chữ số nhị phân. Một bit tương ứng với một sự kiện có hai khả năng xảy ra. Đơn cử như việc một bóng đèn có 2 trạng thái là
- Tối khi tắt công tắc điện (ứng với \\(0\\)).
- Sáng khi mở công tắc điện (ứng với \\(1\\)).

Số học nhị phân sử dụng hai ký số \\(0\\) và \\(1\\) để biểu diễn các số. Vì khả năng sử dụng hai số \\(0\\) và \\(1\\) là như nhau nên một chỉ thị gồm một chữ số nhị phân có thể xem như là đơn vị chứa thông tin nhỏ nhất.

| Tên gọi | Ký hiệu | Giá trị    |  
|:--------|:-------:|-----------:|
|Bit	  |b	    |Binary Digit|
|Byte	  |B        |	8 bit    |
|Kilobyte |KB	    |\\(2^{10}\\)B|
|Megabyte |MB	    |\\(2^{20}\\)B|
|Gigabyte |GB	    |\\(2^{30}\\)B|
|Terabyte |TB	    |\\(2^{40}\\)B|
|Petabyte |PB	    |\\(2^{50}\\)B|
|Exabyte  |EB	    |\\(2^{60}\\)B|
|Zettabyte|ZB	    |\\(2^{70}\\)B|
|Yottabyte|YB	    |\\(2^{80}\\)B|
|Brontobyte|BB	    |\\(2^{90}\\)B|
|Geopbyte |GeB      |\\(2^{100}\\)B|

Cùng xét ví dụ sau: Một người chọn ngẫu nhiên họ và tên sinh viên trong danh sách \\(8\\) người (liệt kê, có thứ tự). Xác suất để mỗi người được chọn là như nhau và cùng bằng \\(\dfrac{1}{8}\\). Để biết người đó chọn ai, ta có thể hỏi bằng YES/NO, như sau
- Sinh viên được chọn có trong 4 người đầu không?

Nếu câu trả lời là có, thì ta tiếp tục hỏi như vậy với 2 người đầu trong 4 người đâu, ngược lại ta sẽ hỏi với 2 người đầu trong 4 người sau. Dễ thấy, chỉ với \\(3 = \log_2 8\\) câu hỏi, ta sẽ xác định người được chọn.  Số \\(3\\) ở đây chính là lượng thông tin tối thiểu (\\(3\\) bits) cần để xác định sinh viên đó.

Một ví dụ khác: Trên đường có \\(4\\) loại xe lưu thông là xe đạp, xe mô tô, xe ô tô con và xe bus. Ở một trạm kiểm soát giao thông, dựa vào tần xuất xe qua trạm, người ta thấy rằng trong một đơn vị thời gian, xác suất để xuất hiện xe đạp, xe mô tô, xe ô tô con và xe bus lần lượt là \\(\dfrac{1}{2}, \dfrac{1}{4}, \dfrac{1}{8}, \dfrac{1}{8}\\). Cần tốn bao nhiêu câu hỏi nhị phân để biết được xe loại nào đang qua trạm?
Giả thiết trên có thể được mô tả bằng biểu đồ sau
{% include aligner.html images="dl_posts/entropy/xe.png" %}
Từ ví dụ trước, có thể thấy rằng với xác suất \\(\dfrac{1}{2^n}\\), cần \\(n\\) câu hỏi (bits thông tin). Như vậy số câu hỏi cần dùng cho mỗi loại xe là

| Xe đạp | Xe mô tô | Xe ô tô con | Xe bus |
|:------:|:--------:|:-----------:|:-------:|
| 1      |     2    |      3      |       3 |

Do đó, số câu hỏi kỳ vọng sẽ là
\\[
    1\times \dfrac{1}{2} + 2\times \dfrac{1}{4} + 3\times \dfrac{1}{8} + 3\times \dfrac{1}{8} = 1.75
\\]
Qua ví dụ trên, có thể rút ngay được nhận xét
- Lượng thông tin của một tin tỉ lệ thuận với số khả năng xảy ra và tỉ lệ nghịch với xác suất xuất hiện của tin đó.
- Thông tin nào mà càng có khả năng xảy ra (xác suất xảy ra lớn) thì thông tin đó ít có giá trị (do lượng thông tin mang lại ít), điều này cũng tương đối dễ hiểu: Cũng như việc ta biết xác suất độ tuổi của một người nằm trong khoảng \\(\left(0;,100\right)\\) rất gần \\(1\\). Do đó, thông tin này có thể coi là không mang giá trị.

Việc phải "feed" một lượng thông tin (qua các câu hỏi YES/NO) để nhận được thông tin về người được chọn, có vẻ giống với "kẻ mà ai cũng biết, nhưng không thèm chấp!" - `entropy`.

Ta tìm hiểu về độ đo `self-information` trước tiên.

**Self-information** cho phép ta mô tả lượng thông tin mà ta thu được từ một kết quả cụ thể của một sự kiện. Một cách trực giác, nếu một kết quả của sự kiện càng làm cho ta ngạc nhiên (có tính bất ngờ lớn), thì ta thu được càng nhiều thông tin hơn. Chúng ta thấy "độ bất ngờ" có vẻ như là một độ đo tốt cho định lượng thông tin. Do đó, định nghĩa của self-information được xây dựng từ xác suất của một sự kiện.
> Self-information function (hàm lượng thông tin) là một ánh xạ \\(I\\) như sau
\\[
    I: [0,1]\rightarrow[0,+\infty] \text{ với } I(p) = \log_a \dfrac{1}{p} = -\log_a p,
\\]
trong đó với \\(a\\) là cơ số được chọn dựa trên đơn vị thông tin sử dụng và \\(p\in [0,1]\\). Entropy thông tin (còn gọi entropy nhị phân) là hàm entropy với cơ số \\(a=2\\).

<img src="{% link _dl/self-information.png %}" text-align="center">

**Điểm qua về ý tưởng**

- Do tính chất của thông tin, Self-information phải là một hàm liên tục.
- Độ đo của Self-information phải không âm.
- Xác suất nhỏ thì độ bất ngờ lớn, hay \\[p_1<p_2 \Rightarrow I(p_1)>I(p_2).\\]
- Độ đo của thông tin có xác suất chắc chắn là \\(1\\) phải bằng \\(0\\), hay \\(I(p=1)=0\\).
- Nếu hai biến cố \\(A, B\\) là độc lập, \\(p_A<p_B\\) thì \\(I(p_A\cdot p_B) = I(p_A) + I(p_B)\\). Có thể hiểu đơn giản bằng ví dụ sau

    - Hai bóng đèn \\(A, B\\) trong phòng có xác suất hỏng trong ngày lần lượt là \\(0.2\\) và \\(0.3\\), đương nhiên hai bóng này là độc lập với nhau. Như vậy nếu gọi \\(C\\) là xác suất cả hai bóng đều hỏng trong ngày thì \\(P(C)=P(A\cap B) = P(A)\cdot P(B)\\). Đương nhiên
    \\[\text{thông tin "hai bóng đều hỏng"} = \text{thông tin "bóng \\(A\\) hỏng" + thông tin "bóng \\(B\\) hỏng"}.
    \\] 

Với kiến thức trên, ta đi vào tìm hiểu độ đo `entropy`.
<!-- Đồ thị trên nhìn giống với hàm loss trong giải thuật Machine Learning bạn nhỉ? Vậy có mối liên hệ nào giữa chúng không? -->

> Một độ đo cơ bản của thông tin là `entropy`, thường được diễn đạt dưới dạng số lượng bit cần thiết trung bình để lưu trữ hoặc dẫn truyền. Entropy lượng hóa sự không chắc chắn trong việc dự đoán giá trị của một biến ngẫu nhiên. 

> Nếu \\(\mathbb{X}\\) là tập hợp tất cả các thông điệp \\(\left\lbrace x_1, x_2,\ldots, x_n\right\rbrace\\) mà \\(X\\) có thể nhận giá trị, và \\(P(x)\\) là xác suất \\(X\\) nhận giá trị \\(x\in \mathbb{X}\\), thì entropy của \\(X\\) được định nghĩa như sau
\\[
H(X)=E_X [I(P(X=x))] =\sum_{x\in \mathbb {X}}P(X=x)\cdot I(P(X=x)),
\\], hay
\\[
    H(X) = -\sum_{x\in \mathbb {X}}P(X=x)\log_a P(X=x).
\\]

- Ví dụ: Khi ta xác định kết quả của một lần tung đồng xu công bằng (hai kết quả có khả năng như nhau) cho ít thông tin hơn (entropy nhỏ hơn) là xác định kết quả của một lần tung xúc sắc (sáu kết quả có khả năng như nhau).
- entropy của ví dụ 4 loại xe chính là \\(1.75\\).
- Về bản chất entropy chính là **trung bình thông tin** của các biến ngẫu nhiên rời rạc!
- entropy đạt cực đại khi phân bố xác suất là phân bố đều, \\(H(X) \leq \log _{a}(n)\\), dấu đẳng thức xảy ra khi \\(p_i = P(X=x_i)=\dfrac{1}{n}, \forall i = \overline{1,n}\\).

Khi Shannon nghĩ ra công thức đo lượng thông tin (hay là lượng thông tin bị mất mát đi khi truyền tin) vào quãng những năm 1944-1948, định gọi nó là “độ bất xác định”, nhưng [John von Neumann](https://en.wikipedia.org/wiki/John_von_Neumann) đã khuyên nên đặt tên nó là entropy, với hai lý do: Một là công thức của Shannon trông giống y chang công thức entropy trong vật lý thống kê (như kiểu công thức của Boltzmann), và hai là “không ai thực sự biết entropy là gì, nên cậu gọi như vậy thì sẽ lợi thế khi tranh luận!”. Vậy là cái tên **entropy** trong lý thuyết thông tin ra đời. Mở ra một kỷ nguyên mới cho ngành thông tin, mật mã,... mà gần đây nhất là Machine Learning.

# Về cross-entropy trong Machine Learning
> Độ đo cross-entropy giữa hai phân bố xác suất rời rạc (discrete probability distribution) \\(P, Q\\) với các véc tơ xác suất \\(p=(p_1,p_2,\ldots,p_n), q = (q_1, q_2,\ldots, q_n)\\) được định nghĩa
\\[
H(p, q)=-\sum_{i=1}^{n} p_{i} \log_a q_{i}
\\]
> Độ đo cross-entropy giữa hai phân bố xác suất liên tục (continuous probability distribution) \\(P, Q\\) với không gian mẫu \\(\mathbb{X}\\) liên tục
\\[
H(p, q)=-\int_{\mathbb{X}} p(x) \log_a q(x) d(x)
\\]

Như vậy, cross-entropy đóng vai trò là độ đo sự khác biệt của hai phân bố xác suất.

Ở bài này, tôi chỉ thông qua các tính chất của cross-entropy với phân bố rời rạc.

**Một số tính chất**
- \\(H(p, q)\ge H(p)\\). Chứng minh điều này khá đơn giản như sau
\\[
H(p, q)=-\sum_{x} p(x) \log_a \left(\frac{q(x) p(x)}{p(x)}\right)=-\sum_{x} p(x) \log_a \left(\frac{q(x)}{p(x)}\right)-\sum_{x} p(x) \log_a p(x).
\\] 
Dễ thấy với \\(a>1\\) và \\(u\in (0,1]\\) thì \\(\log_a u \le u-1\\), do đó
\\[
-\sum_{x} p(x) \log_a \left(\frac{q(x)}{p(x)}\right) \ge \sum_{x} p(x)\left(1-\frac{q(x)}{p(x)}\right)=\sum_{x} p(x)-\sum_{x} q(x)=1-\sum_{x} q(x) \geq 0.
\\]
Từ đây \\(H(p,q)\ge -\sum_{x} p(x) \log _{a} p(x) = H(p)\\).
- cross-entropy không có tính đối xứng do \\(H(p,q)\ne H(q,p)\\) nên nó không phải là một khoảng cách mêtric. Trong các bài toán thì độ đọ \\(H(p,q)\\) thường được dùng với \\(p\\) là phân bố đùng cần dự đoán, phân bố \\(q\\) là phân bố mà mô hình hiện tại đang dự đoán, tối tiểu hàm này để cải thiện mô hình.
- cross-entropy khi dùng như hàm mất mát, hàm này phạt rất nặng khi xác suất \\(p_i\\) lớn nhưng \\(q_i\\) nhỏ, lý do là hàm \\(-\log_b x\\) tăng rất nhanh khi \\(x\\) tiến về \\(0\\). Có thể hiểu qua ví dụ sau.

Giả sử đầu ra của bài toán phân loại chó hoặc không phải chó \\(5\\) ảnh (dòng 2 là xác suất bức ảnh là chó mà mô hình đưa ra, dòng 3 là nhãn cho ảnh thật (1: chó, 0: không phải chó)). 

| image 1 | image 2 | image 3 | image 4 | image 5 |
|:-------:|:-------:|:-------:|:-------:|:-------:|
|  0.7654 |  0.1223 |  0.9645 |  0.6319 |  0.3614 |
|    1    |    0    |    1    |    1    |    0    |

```python
import numpy as np 
import matplotlib.pyplot as plt 

q = np.array([[0.7654, 1-0.7654],
			 [0.1223, 1-0.1223],
			 [0.9645, 1-0.9645],
			 [0.6319, 1-0.6319],
			 [0.3614, 1-0.3614]]
			 )

p = np.array([[1,0],
			  [0,1],
			  [1,0],
			  [1,0],
			  [0,1]]
			  )

def cross_entropy(p,q):
	n = len(p)
	return - np.sum(p*np.log(q))/n

def MSE(p,q):
	n = len(p)
	return np.sum([(p[i]-q[i])**2 for i in range(len(p))])/n

print("cross entropy loss: ",cross_entropy(p,q))
print("mse loss: ",quadratic(p,q))
```
Kết quả thu được
```python
cross entropy loss:  0.2682907411830272
mse loss:  0.13494490799999997
```
Từ đó, thấy rằng loss của cross entropy cao hơn so với loss của mse, tức là cross entropy phạt nặng hơn so với mse. Xin mở rộng thêm bằng minh họa sau (ảnh lấy từ [mlcb-softmax](https://machinelearningcoban.com/2017/02/17/softmax/))

<img src="{% link _dl/cross_entropy.png %}" text-align="center">

Như vậy, có thể rút ra một nhận xét: cross-entropy là ha

Nếu có thời gian, tôi sẽ sửa và viết tiếp bài này, các phần tôi muốn viết là
1. Độ đo Kullback–Leibler divergence và các vấn đề liên quan
2. So sánh cross-entropy với Hinge, Huber, Kullback–Leibler, MAE, dữ liệu là ảnh thật (chó, mèo,...)

Cảm ơn bạn đã đọc đến đây, nếu có nhận xét hay góp ý nào, bạn comment ở dưới hoặc thông tin cho tôi qua mail nhé. Mến chào bạn.

# Tài liệu tham khảo
[1] [Ly thuyet thong tin](https://thetalog.com/statistics/ly-thuyet-thong-tin/)

[2] [Information theory](https://en.wikipedia.org/wiki/Entropy_(information_theory))

[3] [Entropy](http://zung.zetamu.net/2010/09/entropy/#:~:text=Theo%20g%E1%BB%91c%20Hy%20L%E1%BA%A1p%2C%20%E2%80%9Ctropy,ch%E1%BB%AF%20tr%C3%B4%20%C4%91%E1%BB%8Dc%20ch%E1%BB%9D%20n%E1%BA%B7ng)

[4] [Entropy mlcb](https://machinelearningcoban.com/2017/02/17/softmax/)

[5] [A Review of the Entropy Concept](https://arxiv.org/pdf/1711.07326.pdf#:~:text=Roots%20and%20genesis%20of%20the%20entropy%20concept&text=Two%20kinds%20of%20entropy%3B%20thermodynamic,above%20absolute%20zero%20%5B117%5D.)

[6] [Câu chuyện đằng sau sự ra đời của kỷ nguyên thông tin – I](https://dzuykhanh.wordpress.com/2018/04/07/claude-shannon/)

[7] [Từ entropy đến Đạo](https://viethungpham.com/2015/06/09/from-entropy-to-the-tao-tu-entropy-den-dao/)